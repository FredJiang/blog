---
title: 线性回归
date: 2017-07-20 07:09:18
tags: [machine learning]
---

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


## 一个变量

一组训练数据

$$ (x, y) $$ 

第 i 组训练数据

$$ ( x^{(i)}, y^{(i)} ) $$

<!--more-->

当只有一个变量，hypothesis 为

$$ h_\theta(x)=\theta_0 + \theta_1x $$


parameters 为

$$ \theta_0, \theta_1 $$


cost function

$$
\begin{align*} 
J(\theta_0, \theta_1) 
& = \frac{1}{2m} \sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)} ) }^2 \\
& = \frac{1}{2m} \sum_{i=1}^m {(\theta_0 + \theta_1x^{(i)} - y^{(i)} ) }^2 \\
& = \frac{1}{2m} \sum_{i=1}^m (\theta_0^2 + {(\theta_1x^{(i)})}^2 + {(y^{(i)})}^2 + 2 \theta_0 \theta_1x^{(i)} - 2 \theta_0 y^{(i)} - 2 \theta_1x^{(i)} y^{(i)} ) \\
\end{align*} 
$$

cost function 的来源依据为

* [最小二乘法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95) 
* [最小二乘法](https://baike.baidu.com/item/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95)

这里的 `$ \frac{1}{2m} $` 好像是没有特别的意思，只是为了方便计算后面的东西

这里假设 `$ \theta_0 $` 为 0，也就是 `$ h_\theta(x) $` 截距为 0 时

函数 `$ h_\theta(x) $` 和 `$ J(\theta_1) $` 如下图所示

这里假设训练数据有 (1, 1) (2, 2) (3, 3)

{% asset_img "jtheta.png" "" %}



根据 cost function 的定义，要使得 `$ J(\theta_1) $` 最小，就得找到对应的 `$ \theta_1 $`，最终的公式如下：

$$ \theta_1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) $$


从计算机系同学的角度看，写成这样，可能更亲切一点，就是累加累减的意思

$$ \theta_1 -= \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) $$

那这个公式怎么理解呢？


表达式 `$ \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) $` 是 `$ J(\theta_0, \theta_1) $` 对 `$ \theta_1 $` 的偏导数，也就是上图中 `$ J(\theta_1) $` 的斜率


假设 `$ \theta_1 $` 从 B 点开始，由上图可知这个点的斜率为正数，`$ \theta_1 $` 减去一个 `$ \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) $`，说明 `$ \theta_1 $` 要往左移动，才能向 E 点靠近，这里的 `$ \alpha $` 是一个用来调整向 E 点靠近的速度的参数。另外向 E 点靠近的速度会越来越慢，因为斜率越来越小，当斜率为 0 时，说明到达最低点





那么现在的问题是：表达式 `$ \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) $` 怎么求？

<https://www.coursera.org/learn/machine-learning/lecture/kCvQc/gradient-descent-for-linear-regression>


先把结果给各位看官亮出来吧

$$
\begin{align*} 
& \text{repeat until convergence:} \{ \\
& \ \ \ \ \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y^{(i)}) \\
& \ \ \ \ \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y^{(i)}) \cdot x^{(i)} \\
& \} \\
\end{align*}
$$




过程如下


$$
\begin{align*}
J(\theta_0, \theta_1) 
& = \frac{1}{2m} \sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)} ) }^2 \\
& = \frac{1}{2m} \sum_{i=1}^m {(\theta_0 + \theta_1x^{(i)} - y^{(i)} ) }^2 \\
& = \frac{1}{2m} \sum_{i=1}^m (
\theta_0^2 
+ {(\theta_1x^{(i)})}^2 
+ {(y^{(i)})}^2 
+ 2 \theta_0 \theta_1x^{(i)} 
- 2 \theta_0 y^{(i)} 
- 2 \theta_1x^{(i)} y^{(i)} ) \\
\end{align*}
$$


函数 `$ J(\theta_0, \theta_1) $` 对 `$ \theta_0 $` 求偏导


$$
\begin{align*}
\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)
& = \frac{1}{2m} \sum_{i=1}^m (2\theta_0 + 2 \theta_1x^{(i)} - 2  y^{(i)} ) \\
& = \frac{1}{m} \sum_{i=1}^m (\theta_0 +  \theta_1x^{(i)} -   y^{(i)} ) \\
& = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) -   y^{(i)} ) \\
\end{align*}
$$


函数 `$ J(\theta_0, \theta_1) $` 对 `$ \theta_1 $` 求偏导



$$
\begin{align*}
\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)
& = \frac{1}{2m}  \sum_{i=1}^m (2\theta_1 (x^{(i)})^2+ 2 \theta_0 x^{(i)} - 2 x^{(i)} y^{(i)} ) \\
& = \frac{1}{m} \sum_{i=1}^m (\theta_1  (x^{(i)})^2 +  \theta_0 x^{(i)} -  x^{(i)} y^{(i)} ) \\
& = \frac{1}{m} \sum_{i=1}^m (\theta_1 x^{(i)} +  \theta_0  -   y^{(i)} ) x^{(i)} \\
& = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) -   y^{(i)} ) x^{(i)} \\
\end{align*}
$$



另外在 <https://www.coursera.org/learn/machine-learning/supplement/U90DX/gradient-descent-for-linear-regression>

还有这么一个公式


$$
\begin{align*}
\frac{\partial}{\partial \theta_j} J(\theta)
& = \frac{\partial}{\partial \theta_j} \frac{1}{2}       {(h_\theta(x) - y)}^2 \\
& = 2 \cdot \frac{1}{2} (h_\theta(x) - y) \cdot    \frac{\partial}{\partial \theta_j}      (h_\theta(x) - y) \\
& =  (h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_j} \sum_{i=0}^m (\theta_i x_i -y) \\
& =  (h_\theta (x) - y) x_j
\end{align*}
$$




其中

$$
\begin{align*}

h_\theta(x) 
& = \theta_0 x_0 + \theta_1x_1 + \cdots + \theta_nx_n \\

\frac{\partial}{\partial \theta_j} \sum_{i=0}^n (\theta_i x_i -y) 
& = \frac{\partial}{\partial \theta_j} (\theta_0 x_0 + \theta_1 x_1 + \theta_i x_i + \theta_j x_j + \cdots + \theta_n x_n  -y)  \\
& = x_j


\end{align*}
$$



当有多个变量时，得到如下公式

$$ J(\theta_0, \theta_1, \cdots, \theta_n) = \frac{1}{2m} \sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)} ) }^2 $$

将式子展开求偏导数，另外还有 `$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) $`，得到

$$
\begin{align*} 
& \text{repeat} \{ \\
& \ \ \ \ \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \\
& \} \\
\end{align*}
$$





## 多个变量

最后要得出这么一个公式

$$ \vec \theta = {(X^TX)}^{-1} X^T \vec y $$


{% asset_img "multiple_features.png" "" %}



先根据上图先确定几个变量


$$
x^{(2)} = 
\begin{bmatrix} 
1416 \\
3 \\
2 \\
40
\end{bmatrix}
$$


$$
x_3^{(2)} = 2
$$

hypothesis

$$ h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \cdots + \theta_nx_n = \sum_{i=0}^{n}\theta_ix_i = \theta^T X $$

一般设置 `$ x_0 $` 为 1，得到

$$ h_\theta(x) = \theta_0 + \theta_1x_1 + \cdots + \theta_nx_n = \sum_{i=0}^{n}\theta_ix_i = \theta^T X $$


其中

$$
X = 
\begin{bmatrix} 
x_0 \\
x_1 \\
x_2 \\
{\vdots} \\
x_n
\end{bmatrix}
$$

$$
\theta = 
\begin{bmatrix} 
\theta_0 \\
\theta_1 \\
\theta_2 \\
{\vdots} \\
\theta_n
\end{bmatrix}
$$





cost function

$$
\begin{align*} 
J(\theta) 
& = \frac{1}{2m} \sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)} ) }^2 \\
& = \frac{1}{2m} \sum_{i=1}^m { (\theta_0x_0^{(i)} + \theta_1x_1^{(i)} + \cdots + \theta_mx_m^{(i)}  - y^{(i)} ) }^2 \\
& = \frac{1}{2m} \sum_{i=1}^m { (\theta^T X - y^{(i)} ) }^2 \\
\end{align*} 
$$


由公式

$$
\begin{align*} 
\sum_{i=1}^n {(z_i)}^2 
& = z_i z_i \\
& = z_1 z_1 + z_2 z_2 +  \cdots  + z_i z_i  \\
& = z^{T} z
\end{align*}  
$$


得出


$$
\begin{align*} 
J(\theta) 
& = \frac{1}{2m} \sum_{i=1}^m { (\theta^T X - y^{(i)} ) }^2 \\
& = \frac{1}{2m} { (\theta^T X - y^{(i)} ) }^T  { (\theta^T X - y^{(i)} ) }
\end{align*} 
$$


最后能得出

$$ \vec \theta = {(X^TX)}^{-1} X^T \vec y $$

这里涉及到了矩阵的偏导数，可以参考一下文章

* <http://www.jianshu.com/p/7c4fda4f1498>
* <http://www.solinx.co/archives/721>
* <http://xuehy.github.io/2014/04/18/2014-04-18-matrixcalc/>
* <https://zh.wikipedia.org/wiki/%E8%BD%AC%E7%BD%AE%E7%9F%A9%E9%98%B5>
* [逆矩阵](https://zh.wikipedia.org/wiki/%E9%80%86%E7%9F%A9%E9%98%B5)
* [伴随矩阵](https://zh.wikipedia.org/wiki/%E4%BC%B4%E9%9A%8F%E7%9F%A9%E9%98%B5)


$$
x^{(i)} = 
\begin{bmatrix} 
x_0^{(i)} \\
x_1^{(i)} \\
x_2^{(i)} \\
\vdots \\
x_n^{(i)} \\
\end{bmatrix}
$$



$$
x = 
\begin{bmatrix} 
{(x^{(0)})}^T \\
{(x^{(1)})}^T \\
{(x^{(2)})}^T \\
\vdots \\
{(x^{(m)})}^T \\
\end{bmatrix}
= 
\begin{bmatrix} 
x_0^{(0)} & x_1^{(0)} & x_2^{(0)} & \dots  & x_n^{(0)} & \\
x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & \dots  & x_n^{(1)} & \\
x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & \dots  & x_n^{(2)} & \\
\vdots    & \vdots    & \vdots    & \vdots & \vdots    & \\
x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & \dots  & x_n^{(m)}
\end{bmatrix}
$$



$$
x^{(i)} = 
\begin{bmatrix} 
1 \\
x_1^{(i)} \\
\end{bmatrix}
$$



$$
X = 
\begin{bmatrix} 
1 & x_1^{(1)}\\
1 & x_1^{(2)}\\
\vdots & \vdots \\
1 & x_1^{(m)}\\
\end{bmatrix}
$$

$$
y = 
\begin{bmatrix} 
y^{(1)}\\
y^{(2)}\\
\vdots \\
y^{(m)}\\
\end{bmatrix}
$$




## 选变量的技巧


{% asset_img "feature_scaleing.png" "" %}


上图的意思是：
* 变量的范围尽量调整到一样
* 左边的要扁，从上或下走到中间需要更多的时间
* 变量范围最好为 `-1 <= x_i <= 1`



{% asset_img "mean_normalization.png" "" %}

$$
\begin{align*} 
x_i 
& := \frac{x_i - u } {s} \\
& := \frac{x_i - \frac{ \sum_{i=1}^m x_j }{m} } {x_{max} - x_{min}}
\end{align*}
$$


{% asset_img "alpha.png" "" %}



* <https://machinelearningmastery.com/gradient-descent-for-machine-learning/>

