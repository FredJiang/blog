---
title: 线性回归
date: 2017-07-20 07:09:18
tags: [machine learning]
---

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




一组训练数据

$$ (x, y) $$ 

第 i 组训练数据

$$ ( x^{(i)}, y^{(i)} ) $$



hypothesis

$$ h_\theta(x)=\theta_0 + \theta_1x $$


parameters

$$ \theta_0, \theta_1 $$


cost function

$$ J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)} ) }^2 $$


$$ h_\theta(x) $$

参数为 x

截距为 0 时

$$ J(\theta_1) $$


参数为 
$$ \theta_1 $$



$$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)  $$





hypothesis

$$ h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \cdots + \theta_nx_n = \sum_{i=0}^{n}\theta_ix_i = \theta^T\vec{X} $$


在只有一个属性的时候

$$ h_\theta(x) = \theta_0x_0 + \theta_1x_1 = \sum_{i=0}^{n}\theta_ix_i = \theta^T\vec{X} $$


令 $$ x_0 $$  = 1

在只有一个属性的时候，两组数据


$$ h_\theta(x) = \theta_0 + \theta_1x_1 = \sum_{i=0}^{n}\theta_ix_i = \theta^T\vec{X} $$

$$ h_\theta(x) = \theta_0 + \theta_1x_1 = \sum_{i=0}^{n}\theta_ix_i = {\begin{bmatrix} \theta_1 \\ \theta_2 \end{bmatrix}} ^T {\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}} $$

$$ h_\theta(x) = \theta_0 + \theta_1x_1 = \sum_{i=0}^{n}\theta_ix_i = {\begin{bmatrix} \theta_1 &  \theta_2 \end{bmatrix}}    {\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}} $$




cost function

$$ J(\theta) = \frac{1}{2}\sum_{i=1}^m {(h_\theta(x^{(i)}) - y^{(i)})}^2 $$

